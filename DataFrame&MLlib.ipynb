{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  動機 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>這學期因為上資科系“數據科學與大數據分析”這門課而接觸到了Apache Spark，Spark原本使用的語言是Scala，也有Python的API叫PySpark，因為一直以來都是在使用Python作為分析的語言，就決定用Pyspark，在implement分析的過程中遇到了一點困難，因為Spark網站的tutorial對我來說有一點不直觀，然後也沒發現太多用PySpark做的分析，所以想說自己寫一個完整的PySpark分析流程。 </br>\n",
    "<br></br>\n",
    "<br>系統：Window 7 64bit, Python2.7, Anaconda2 2.4</br>\n",
    "<br>Spark安裝參考：http://dataxscience.blogspot.tw/2016/02/setup-apache-spark-on-windows.html </br>\n",
    "<br>Reference Code：http://www.techpoweredmath.com/spark-dataframes-mllib-tutorial/</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#在Notebook上把spark import進來\n",
    "import os \n",
    "import sys\n",
    "\n",
    "os.environ['SPARK_HOME']=\"C:/spark-1.6.1/\" #看spark在你電腦的path是在哪裡\n",
    "\n",
    "sys.path.append(\"C:/spark-1.6.1/bin\") \n",
    "sys.path.append(\"C:/spark-1.6.1/python\")\n",
    "sys.path.append(\"C:/spark-1.6.1/python/pyspark/\")\n",
    "sys.path.append(\"C:/spark-1.6.1/python/lib\") \n",
    "sys.path.append(\"C:/spark-1.6.1/python/lib/pyspark.zip\")\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]') \n",
    "sqlContext = pyspark.sql.SQLContext(sc) #DataFrame需要用到的SQL context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何在Spark把csv檔用DataFrame的方式呈現?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了要把csv的資料在Spark用DataFrame的方式實現，需要用到pyspark裡面的sql函數套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Spark裡面的資料格式是RDD(Resillient Distributed Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('realestate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用\"take\"來呈現前5行row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'street,city,zip,state,beds,baths,sq__ft,type,sale_date,price,latitude,longitude',\n",
       " u'3526 HIGH ST,SACRAMENTO,95838,CA,2,1,836,Residential,Wed May 21 00:00:00 EDT 2008,59222,38.631913,-121.434879',\n",
       " u'51 OMAHA CT,SACRAMENTO,95823,CA,3,1,1167,Residential,Wed May 21 00:00:00 EDT 2008,68212,38.478902,-121.431028',\n",
       " u'2796 BRANCH ST,SACRAMENTO,95815,CA,2,1,796,Residential,Wed May 21 00:00:00 EDT 2008,68880,38.618305,-121.443839',\n",
       " u'2805 JANETTE WAY,SACRAMENTO,95815,CA,2,1,852,Residential,Wed May 21 00:00:00 EDT 2008,69307,38.616835,-121.439146']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從上面的結果可以看到其實每一row的資料是還沒有用\",\"區隔開的，我們可以針對每一row做\"split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'street',\n",
       "  u'city',\n",
       "  u'zip',\n",
       "  u'state',\n",
       "  u'beds',\n",
       "  u'baths',\n",
       "  u'sq__ft',\n",
       "  u'type',\n",
       "  u'sale_date',\n",
       "  u'price',\n",
       "  u'latitude',\n",
       "  u'longitude'],\n",
       " [u'3526 HIGH ST',\n",
       "  u'SACRAMENTO',\n",
       "  u'95838',\n",
       "  u'CA',\n",
       "  u'2',\n",
       "  u'1',\n",
       "  u'836',\n",
       "  u'Residential',\n",
       "  u'Wed May 21 00:00:00 EDT 2008',\n",
       "  u'59222',\n",
       "  u'38.631913',\n",
       "  u'-121.434879']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "據我所知，目前好像沒有方法可以讓Spark看懂標題，所以我們先用\"filter\"把標題移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = rdd.first()\n",
    "rdd = rdd.filter(lambda line:line != header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>接下來就是把Spark的RDD轉換成DataFrame的方法咯!</br>\n",
    "<br>轉換成DataFrame的好處是我們可以選取我們有興趣的列，當然如果你有很多的features那可能就沒有太多的差別</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = rdd.map(lambda line: Row(street = line[0],\n",
    "                              city = line[1],\n",
    "                              zip = line[2],\n",
    "                              beds = float(line[4]),\n",
    "                              baths = float(line[5]),\n",
    "                              sqft = float(line[6]),\n",
    "                              price = float(line[9]))).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "雖然DataFrame也可以用\"take\"的方式來觀察資料，但其實可讀性並不高，在這裡我們用\"show\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+-------+------+----------------+-----+\n",
      "|baths|beds|      city|  price|  sqft|          street|  zip|\n",
      "+-----+----+----------+-------+------+----------------+-----+\n",
      "|  1.0| 2.0|SACRAMENTO|59222.0| 836.0|    3526 HIGH ST|95838|\n",
      "|  1.0| 3.0|SACRAMENTO|68212.0|1167.0|     51 OMAHA CT|95823|\n",
      "|  1.0| 2.0|SACRAMENTO|68880.0| 796.0|  2796 BRANCH ST|95815|\n",
      "|  1.0| 2.0|SACRAMENTO|69307.0| 852.0|2805 JANETTE WAY|95815|\n",
      "|  1.0| 2.0|SACRAMENTO|81900.0| 797.0| 6001 MCMAHON DR|95824|\n",
      "+-----+----+----------+-------+------+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas & DataFrame Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在就可以開始用DataFrame中的操作方式咯!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baths</th>\n",
       "      <th>beds</th>\n",
       "      <th>city</th>\n",
       "      <th>price</th>\n",
       "      <th>sqft</th>\n",
       "      <th>street</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SACRAMENTO</td>\n",
       "      <td>59222.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>3526 HIGH ST</td>\n",
       "      <td>95838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>SACRAMENTO</td>\n",
       "      <td>68212.0</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>51 OMAHA CT</td>\n",
       "      <td>95823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SACRAMENTO</td>\n",
       "      <td>68880.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>2796 BRANCH ST</td>\n",
       "      <td>95815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SACRAMENTO</td>\n",
       "      <td>69307.0</td>\n",
       "      <td>852.0</td>\n",
       "      <td>2805 JANETTE WAY</td>\n",
       "      <td>95815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SACRAMENTO</td>\n",
       "      <td>81900.0</td>\n",
       "      <td>797.0</td>\n",
       "      <td>6001 MCMAHON DR</td>\n",
       "      <td>95824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   baths  beds        city    price    sqft            street    zip\n",
       "0    1.0   2.0  SACRAMENTO  59222.0   836.0      3526 HIGH ST  95838\n",
       "1    1.0   3.0  SACRAMENTO  68212.0  1167.0       51 OMAHA CT  95823\n",
       "2    1.0   2.0  SACRAMENTO  68880.0   796.0    2796 BRANCH ST  95815\n",
       "3    1.0   2.0  SACRAMENTO  69307.0   852.0  2805 JANETTE WAY  95815\n",
       "4    1.0   2.0  SACRAMENTO  81900.0   797.0   6001 MCMAHON DR  95824"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只抓取zip是95815的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "favorite_zip = df[df.zip == 95815]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+--------+------+----------------+-----+\n",
      "|baths|beds|      city|   price|  sqft|          street|  zip|\n",
      "+-----+----+----------+--------+------+----------------+-----+\n",
      "|  1.0| 2.0|SACRAMENTO| 68880.0| 796.0|  2796 BRANCH ST|95815|\n",
      "|  1.0| 2.0|SACRAMENTO| 69307.0| 852.0|2805 JANETTE WAY|95815|\n",
      "|  1.0| 1.0|SACRAMENTO|106852.0| 871.0| 2930 LA ROSA RD|95815|\n",
      "|  1.0| 2.0|SACRAMENTO| 78000.0| 800.0|    3132 CLAY ST|95815|\n",
      "|  2.0| 4.0|SACRAMENTO| 89000.0|1316.0| 483 ARCADE BLVD|95815|\n",
      "+-----+----+----------+--------+------+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "favorite_zip.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只看city和beds這兩個features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+\n",
      "|          city|beds|\n",
      "+--------------+----+\n",
      "|    SACRAMENTO| 2.0|\n",
      "|    SACRAMENTO| 3.0|\n",
      "|    SACRAMENTO| 2.0|\n",
      "|    SACRAMENTO| 2.0|\n",
      "|    SACRAMENTO| 2.0|\n",
      "|    SACRAMENTO| 3.0|\n",
      "|    SACRAMENTO| 3.0|\n",
      "|    SACRAMENTO| 3.0|\n",
      "|RANCHO CORDOVA| 2.0|\n",
      "|     RIO LINDA| 3.0|\n",
      "+--------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('city','beds').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "針對beds做加總的groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|beds|count|\n",
      "+----+-----+\n",
      "| 1.0|   10|\n",
      "| 6.0|    3|\n",
      "| 3.0|  413|\n",
      "| 5.0|   59|\n",
      "| 8.0|    1|\n",
      "| 4.0|  258|\n",
      "| 0.0|  108|\n",
      "| 2.0|  133|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"beds\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>接下來是pandas裡面一個用來很好觀察資料的函數：describe</br>\n",
    "<br>可以看到有一些不好的資料，例如sqft有出現0，感覺上一間房子sqft為0是有點怪怪的</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|             baths|              beds|             price|              sqft|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|               985|               985|               985|               985|\n",
      "|   mean|1.7766497461928934|2.9116751269035532|234144.26395939087|1314.9167512690356|\n",
      "| stddev| 0.895371422318646|1.3079322320435811|  138365.839084928| 853.0482425034447|\n",
      "|    min|               0.0|               0.0|            1551.0|               0.0|\n",
      "|    max|               5.0|               8.0|          884790.0|            5822.0|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(['baths','beds','price','sqft']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark裡面Machine Learning的套件叫MLlib，我們現在以linear regression為例子，首先import幾個必要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib\n",
    "import pyspark.mllib.regression\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我先創造一個我有興趣的features的dataframe，接下來我將用baths,beds.sqft這三個features去預測房價price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df.select('price','baths','beds','sqft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來把一些不合理的數據刪除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|             baths|              beds|             price|              sqft|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|               814|               814|               814|               814|\n",
      "|   mean|1.9606879606879606|3.2444717444717446| 229448.3697788698|1591.1461916461917|\n",
      "| stddev|0.6698038253879438|0.8521372615281976|119825.57606009026| 663.8419297942894|\n",
      "|    min|               1.0|               1.0|            2000.0|             484.0|\n",
      "|    max|               5.0|               8.0|          884790.0|            5822.0|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data[data.baths > 0]\n",
    "data = data[data.beds > 0]\n",
    "data = data[data.sqft > 0]\n",
    "data.describe(['baths','beds','price','sqft']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Points and Scaling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在MLlib裡面我們需要把features存成LabeledPoints，在這裡的意思是把每一個price和其對映的features組成一個向量。我們用map來做這件事情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(59222.0, [1.0,2.0,836.0]),\n",
       " LabeledPoint(68212.0, [1.0,3.0,1167.0]),\n",
       " LabeledPoint(68880.0, [1.0,2.0,796.0]),\n",
       " LabeledPoint(69307.0, [1.0,2.0,852.0]),\n",
       " LabeledPoint(81900.0, [1.0,2.0,797.0])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = data.map(lambda line: LabeledPoint(line[0],[line[1:]]))\n",
    "temp.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在我們有了MLlib要求的資料格式，已經可以開始建模咯!但是在此之前我們先把資料標準化，因為可以看到sqft和baths,beds的數字相差很大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 2.0, 836.0),\n",
       " (1.0, 3.0, 1167.0),\n",
       " (1.0, 2.0, 796.0),\n",
       " (1.0, 2.0, 852.0),\n",
       " (1.0, 2.0, 797.0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = data.map(lambda row: row[1:])\n",
    "features.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standardizer = StandardScaler()\n",
    "model = standardizer.fit(features)\n",
    "features_transform = model.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "來看看標準化之後資料的樣子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([1.493, 2.347, 1.2593]),\n",
       " DenseVector([1.493, 3.5206, 1.7579]),\n",
       " DenseVector([1.493, 2.347, 1.1991]),\n",
       " DenseVector([1.493, 2.347, 1.2834]),\n",
       " DenseVector([1.493, 2.347, 1.2006])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_transform.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把labels(price)和標準化後的features結合，開始建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59222.0, 68212.0, 68880.0, 69307.0, 81900.0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab = data.map(lambda row: row[0])\n",
    "lab.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用zip的方式結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(59222.0, DenseVector([1.493, 2.347, 1.2593])),\n",
       " (68212.0, DenseVector([1.493, 3.5206, 1.7579])),\n",
       " (68880.0, DenseVector([1.493, 2.347, 1.1991])),\n",
       " (69307.0, DenseVector([1.493, 2.347, 1.2834])),\n",
       " (81900.0, DenseVector([1.493, 2.347, 1.2006]))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformedData = lab.zip(features_transform)\n",
    "transformedData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformedData = transformedData.map(lambda row: LabeledPoint(row[0],row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(59222.0, [1.49297445326,2.34703972035,1.25933593899]),\n",
       " LabeledPoint(68212.0, [1.49297445326,3.52055958053,1.7579486134]),\n",
       " LabeledPoint(68880.0, [1.49297445326,2.34703972035,1.19908063091]),\n",
       " LabeledPoint(69307.0, [1.49297445326,2.34703972035,1.28343806223]),\n",
       " LabeledPoint(81900.0, [1.49297445326,2.34703972035,1.20058701361])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformedData.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib可以很簡單地把資料切割成testing/training的data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingData,testingData = transformedData.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來選擇Stochastic Gradient Descent減少cost function的linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "linearModel = LinearRegressionWithSGD.train(trainingData,1000,.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(59222.0, [1.49297445326,2.34703972035,1.25933593899]),\n",
       " LabeledPoint(68212.0, [1.49297445326,3.52055958053,1.7579486134]),\n",
       " LabeledPoint(68880.0, [1.49297445326,2.34703972035,1.19908063091]),\n",
       " LabeledPoint(69307.0, [1.49297445326,2.34703972035,1.28343806223]),\n",
       " LabeledPoint(81900.0, [1.49297445326,2.34703972035,1.20058701361])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用weights和intercept就可以看到截距和每個features各自的權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DenseVector([15098.627, 3792.023, 70216.8097]), 0.0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearModel.weights,linearModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(100309.0, [2.98594890652,3.52055958053,1.36930187625]),\n",
       " LabeledPoint(124100.0, [2.98594890652,3.52055958053,2.41171870613]),\n",
       " LabeledPoint(148750.0, [2.98594890652,4.69407944071,2.21739533756]),\n",
       " LabeledPoint(150000.0, [1.49297445326,1.17351986018,1.14485085363]),\n",
       " LabeledPoint(161500.0, [2.98594890652,4.69407944071,2.3906293483]),\n",
       " LabeledPoint(166357.0, [1.49297445326,4.69407944071,2.94497818269]),\n",
       " LabeledPoint(168000.0, [2.98594890652,3.52055958053,2.22492725107]),\n",
       " LabeledPoint(178480.0, [2.98594890652,3.52055958053,1.78506350204]),\n",
       " LabeledPoint(181872.0, [1.49297445326,3.52055958053,1.73535287287]),\n",
       " LabeledPoint(182587.0, [4.47892335978,4.69407944071,2.78831438167])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testingData.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict就是用來預測的啦!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154581.78116003965"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearModel.predict([2.98594890652,3.52055958053,1.36930187625])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 檢查Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分類器建好之後接下來就是要來評估分類器的表現咯，這裡我們用RegressionMetrics來實現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把predict的結果和真正的觀察值放在一起準備餵給RegressionMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediObserRDDin = trainingData.map(lambda row: (float(linearModel.predict(row.features)),row.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = RegressionMetrics(prediObserRDDin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "呼叫這個分類器的$R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4969184679643588"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediObserRDDout = testingData.map(lambda row: (float(linearModel.predict(row.features)),row.label))\n",
    "metrics = RegressionMetrics(prediObserRDDout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94895.10434498572"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
